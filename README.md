# Projet_Techniques_Programmation
# Web Crawler
## Introduction
L'objectif de ce projet est de créer un Web Crawler capable de collecter automatiquement des données à partir de sites Web. Le crawler sera créé dans le but d'extraire des données particulières de sites Web et de les stocker dans une base de données ou un fichier local. Après cela, ces données peuvent être examinées, traitées ou appliquées d'autres manières.
#### Crawling le site web d’achat de fils de laine pour faire du tricot
Notre groupe essayons de crawling le site web d’achat de fils de laine pour faire du tricotet et d'utiliser nos codes pour extraire des données sur les fils, telles que la marque,le nom,le prix,le discount et le rating. Les données extraites seront ensuite utilisées pour recommander le bon produit pour l'utilisateur lorsque l'on connaît son budget et ses exigences en matière d'évaluation.
## Environnement de dévéloppement
Toute plate-forme, y compris Windows, macOS ou Linux, qui prend en charge Python peut être utilisée pour construire ce projet Web Crawler. L'installation de Python et d'un éditeur de code ou d'un environnement de développement intégré est nécessaire pour créer un environnement de développement (IDE).
## Paguets de dépendances
Voici quelques paquets utilisés en Python :
1. Requests : Un paquet qui permet d'envoyer des requêtes HTTP aux pages web et d'obtenir leur contenu HTML
2. BeautifulSoup : Un paquet pour l'analyse de documents HTML
3. Selenium : Un paquet pour l'automatisation de navigateurs web
4. Pandas : Un paquet pour la manipulation et l'analyse de données
## Source des données
Le site d'achat que nous avons choisir est le suivant : https://hobbii.fr/fils
## Méthodes principales
1. Utiliser "selenium" qui permet d'effectuer les tests automatisés sur les navigateurs Web 
2. 
## Conclusions

## Applications

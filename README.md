# Projet_Techniques_Programmation
# Web Crawler
## Introduction
L'objectif de ce projet est de créer un Web Crawler capable de collecter automatiquement des données à partir de sites Web. Le crawler sera créé dans le but d'extraire des données particulières de sites Web et de les stocker dans une base de données ou un fichier local. Après cela, ces données peuvent être examinées, traitées ou appliquées d'autres manières.
### Crawling le site web d’achat de fils de laine pour faire du tricot
## Environnement de dévéloppement
Toute plate-forme, y compris Windows, macOS ou Linux, qui prend en charge Python peut être utilisée pour construire ce projet Web Crawler. L'installation de Python et d'un éditeur de code ou d'un environnement de développement intégré est nécessaire pour créer un environnement de développement (IDE).
## Paguets de dépendances
Voici quelques paquets utilisés en Python :
1. Requests : Un paquet qui permet d'envoyer des requêtes HTTP aux pages web et d'obtenir leur contenu HTML
2. BeautifulSoup : Un paquet pour l'analyse de documents HTML
3. Selenium : Un paquet pour l'automatisation de navigateurs web
4. Pandas : Un paquet pour la manipulation et l'analyse de données
## Source des données
## Méthodes principales
## Conclusions
## Applications

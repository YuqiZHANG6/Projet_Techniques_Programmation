# Projet_Techniques_Programmation
# Web Crawler
## Introduction
L'objectif de ce projet est de créer un Web Crawler capable de collecter automatiquement des données à partir de sites Web. Le crawler sera créé dans le but d'extraire des données particulières de sites Web et de les stocker dans une base de données ou un fichier local. Après cela, ces données peuvent être examinées, traitées ou appliquées d'autres manières.
#### Crawling le site web d’achat de fils de laine pour faire du tricot
Notre groupe essayons de crawling le site web d’achat de fils de laine pour faire du tricotet et d'utiliser nos codes pour extraire des données sur les fils, telles que la marque,le nom,le nombre de couleur,le prix,le discount et le rating. Les données extraites seront ensuite utilisées pour recommander le bon produit pour l'utilisateur lorsque l'on connaît son budget et ses exigences en matière d'évaluation.
## Environnement de dévéloppement
Toute plate-forme, y compris Windows, macOS ou Linux, qui prend en charge Python peut être utilisée pour construire ce projet Web Crawler. L'installation de Python et d'un éditeur de code ou d'un environnement de développement intégré est nécessaire pour créer un environnement de développement (IDE).
## Paguets de dépendances
Voici quelques paquets utilisés en Python :
1. BeautifulSoup : Un paquet pour l'analyse de documents HTML
2. Selenium : Un paquet pour l'automatisation de navigateurs web
3. Pandas : Un paquet pour la manipulation et l'analyse de données
## Source des données
Le site d'achat que nous avons choisir est le suivant : https://hobbii.fr/fils
## Méthodes principales
1. Obtenir la source html d'une page web donnée
2. Détecter s'il y a une page suivante
3. Bouclez les pages disponibles et stockez les résultats
4. Organiser et stocker les données dont nous avons besoin
5. Consulter les exigences de l'utilisateur et fournir les résultats correspondants
## Applications
Cet outil est très pratique et peut être appliqué à de nombreux sites d'achat. Lorsque nous voulons acheter un produit mais que nous ne voulons pas parcourir tous les sites web de produits, ce crawler peut nous faire gagner beaucoup de temps.
